{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_outputs),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        return self.softmax(self.layers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Trading_Price</th>\n",
       "      <th>1H_x1_0</th>\n",
       "      <th>1H_x2_0</th>\n",
       "      <th>1H_x3_0</th>\n",
       "      <th>1H_x4_0</th>\n",
       "      <th>1H_x5_0</th>\n",
       "      <th>1H_x1_1</th>\n",
       "      <th>1H_x2_1</th>\n",
       "      <th>1H_x3_1</th>\n",
       "      <th>...</th>\n",
       "      <th>1H_x1_14</th>\n",
       "      <th>1H_x2_14</th>\n",
       "      <th>1H_x3_14</th>\n",
       "      <th>1H_x4_14</th>\n",
       "      <th>1H_x5_14</th>\n",
       "      <th>1H_x1_15</th>\n",
       "      <th>1H_x2_15</th>\n",
       "      <th>1H_x3_15</th>\n",
       "      <th>1H_x4_15</th>\n",
       "      <th>1H_x5_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-20 16:00:00+00:00</td>\n",
       "      <td>1.07353</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-20 17:00:00+00:00</td>\n",
       "      <td>1.07275</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-20 18:00:00+00:00</td>\n",
       "      <td>1.06869</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-20 19:00:00+00:00</td>\n",
       "      <td>1.06588</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-20 20:00:00+00:00</td>\n",
       "      <td>1.06615</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.000741</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  Trading_Price   1H_x1_0   1H_x2_0   1H_x3_0  \\\n",
       "0  2023-09-20 16:00:00+00:00        1.07353  0.001951  0.001138  0.000009   \n",
       "1  2023-09-20 17:00:00+00:00        1.07275 -0.000727 -0.000130  0.001008   \n",
       "2  2023-09-20 18:00:00+00:00        1.06869 -0.003785 -0.000745 -0.004718   \n",
       "3  2023-09-20 19:00:00+00:00        1.06588 -0.002629 -0.003561 -0.001490   \n",
       "4  2023-09-20 20:00:00+00:00        1.06615  0.000253 -0.002498 -0.000741   \n",
       "\n",
       "    1H_x4_0   1H_x5_0   1H_x1_1   1H_x2_1   1H_x3_1  ...  1H_x1_14  1H_x2_14  \\\n",
       "0  0.000149  0.002003 -0.000504 -0.000242  0.000056  ... -0.000299  0.000206   \n",
       "1  0.000746  0.000270  0.001951  0.001138  0.000009  ... -0.000328 -0.000515   \n",
       "2  0.003799  0.001207 -0.000727 -0.000130  0.001008  ... -0.000084 -0.000290   \n",
       "3  0.002861  0.000066 -0.003785 -0.000745 -0.004718  ...  0.000215  0.000262   \n",
       "4  0.000103  0.001060 -0.002629 -0.003561 -0.001490  ...  0.000084  0.000852   \n",
       "\n",
       "   1H_x3_14  1H_x4_14  1H_x5_14  1H_x1_15  1H_x2_15  1H_x3_15  1H_x4_15  \\\n",
       "0  0.000253  0.000627  0.000028  0.000131  0.000159  0.000066  0.000122   \n",
       "1 -0.000421  0.000440  0.000122 -0.000299  0.000206  0.000253  0.000627   \n",
       "2  0.000000  0.000234  0.000037 -0.000328 -0.000515 -0.000421  0.000440   \n",
       "3  0.000009  0.000281  0.000243 -0.000084 -0.000290  0.000000  0.000234   \n",
       "4 -0.000506  0.001048  0.000833  0.000215  0.000262  0.000009  0.000281   \n",
       "\n",
       "   1H_x5_15  \n",
       "0  0.000580  \n",
       "1  0.000028  \n",
       "2  0.000122  \n",
       "3  0.000037  \n",
       "4  0.000243  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Trading_Price</th>\n",
       "      <th>1H_x1_0</th>\n",
       "      <th>1H_x2_0</th>\n",
       "      <th>1H_x3_0</th>\n",
       "      <th>1H_x4_0</th>\n",
       "      <th>1H_x5_0</th>\n",
       "      <th>1H_x1_1</th>\n",
       "      <th>1H_x2_1</th>\n",
       "      <th>1H_x3_1</th>\n",
       "      <th>...</th>\n",
       "      <th>1H_x2_14</th>\n",
       "      <th>1H_x3_14</th>\n",
       "      <th>1H_x4_14</th>\n",
       "      <th>1H_x5_14</th>\n",
       "      <th>1H_x1_15</th>\n",
       "      <th>1H_x2_15</th>\n",
       "      <th>1H_x3_15</th>\n",
       "      <th>1H_x4_15</th>\n",
       "      <th>1H_x5_15</th>\n",
       "      <th>Lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-20 16:00:00+00:00</td>\n",
       "      <td>1.07353</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-20 17:00:00+00:00</td>\n",
       "      <td>1.07275</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-20 18:00:00+00:00</td>\n",
       "      <td>1.06869</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-20 19:00:00+00:00</td>\n",
       "      <td>1.06588</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-20 20:00:00+00:00</td>\n",
       "      <td>1.06615</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.000741</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  Trading_Price   1H_x1_0   1H_x2_0   1H_x3_0  \\\n",
       "0  2023-09-20 16:00:00+00:00        1.07353  0.001951  0.001138  0.000009   \n",
       "1  2023-09-20 17:00:00+00:00        1.07275 -0.000727 -0.000130  0.001008   \n",
       "2  2023-09-20 18:00:00+00:00        1.06869 -0.003785 -0.000745 -0.004718   \n",
       "3  2023-09-20 19:00:00+00:00        1.06588 -0.002629 -0.003561 -0.001490   \n",
       "4  2023-09-20 20:00:00+00:00        1.06615  0.000253 -0.002498 -0.000741   \n",
       "\n",
       "    1H_x4_0   1H_x5_0   1H_x1_1   1H_x2_1   1H_x3_1  ...  1H_x2_14  1H_x3_14  \\\n",
       "0  0.000149  0.002003 -0.000504 -0.000242  0.000056  ...  0.000206  0.000253   \n",
       "1  0.000746  0.000270  0.001951  0.001138  0.000009  ... -0.000515 -0.000421   \n",
       "2  0.003799  0.001207 -0.000727 -0.000130  0.001008  ... -0.000290  0.000000   \n",
       "3  0.002861  0.000066 -0.003785 -0.000745 -0.004718  ...  0.000262  0.000009   \n",
       "4  0.000103  0.001060 -0.002629 -0.003561 -0.001490  ...  0.000852 -0.000506   \n",
       "\n",
       "   1H_x4_14  1H_x5_14  1H_x1_15  1H_x2_15  1H_x3_15  1H_x4_15  1H_x5_15  Lable  \n",
       "0  0.000627  0.000028  0.000131  0.000159  0.000066  0.000122  0.000580      0  \n",
       "1  0.000440  0.000122 -0.000299  0.000206  0.000253  0.000627  0.000028      0  \n",
       "2  0.000234  0.000037 -0.000328 -0.000515 -0.000421  0.000440  0.000122      0  \n",
       "3  0.000281  0.000243 -0.000084 -0.000290  0.000000  0.000234  0.000037      0  \n",
       "4  0.001048  0.000833  0.000215  0.000262  0.000009  0.000281  0.000243      1  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/training/EURUSD.csv\")\n",
    "df[\"Lable\"] = (df[\"Trading_Price\"]-df[\"Trading_Price\"].shift(1)).apply(lambda x: 1 if x > 0 else 0)\n",
    "df = df.dropna()\n",
    "features = th.from_numpy(df.drop([\"Date\", \"Trading_Price\", \"Lable\"], axis=1).to_numpy()).to(device=device).type(th.float32)\n",
    "lable = th.from_numpy(df[\"Lable\"].to_numpy()).to(device=device)\n",
    "lable = nn.functional.one_hot(lable, num_classes=2).type(th.float32)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6254, 80]), torch.Size([6254, 1]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, lable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, lable, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Network(\n",
    "    n_inputs=features.shape[1],\n",
    "    n_outputs=2,\n",
    "    hidden_size=128\n",
    ").to(device=device)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimiser = th.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.6929103136062622 | Accuracy: tensor([0.5180])\n",
      "Epoch: 100 | Loss: 0.6920903921127319 | Accuracy: tensor([0.5140])\n",
      "Epoch: 200 | Loss: 0.6511706113815308 | Accuracy: tensor([0.6867])\n",
      "Epoch: 300 | Loss: 0.5591529607772827 | Accuracy: tensor([0.8601])\n",
      "Epoch: 400 | Loss: 0.49166566133499146 | Accuracy: tensor([0.9440])\n",
      "Epoch: 500 | Loss: 0.4512191414833069 | Accuracy: tensor([0.9592])\n",
      "Epoch: 600 | Loss: 0.42545124888420105 | Accuracy: tensor([0.9664])\n",
      "Epoch: 700 | Loss: 0.4077536165714264 | Accuracy: tensor([0.9752])\n",
      "Epoch: 800 | Loss: 0.3952900767326355 | Accuracy: tensor([0.9680])\n",
      "Epoch: 900 | Loss: 0.3858652710914612 | Accuracy: tensor([0.9768])\n",
      "Epoch: 1000 | Loss: 0.3777618408203125 | Accuracy: tensor([0.9840])\n",
      "Epoch: 1100 | Loss: 0.3717906177043915 | Accuracy: tensor([0.9848])\n",
      "Epoch: 1200 | Loss: 0.3669593036174774 | Accuracy: tensor([0.9848])\n",
      "Epoch: 1300 | Loss: 0.36295583844184875 | Accuracy: tensor([0.9848])\n",
      "Epoch: 1400 | Loss: 0.3602936863899231 | Accuracy: tensor([0.9768])\n",
      "Epoch: 1500 | Loss: 0.3563458323478699 | Accuracy: tensor([0.9888])\n",
      "Epoch: 1600 | Loss: 0.354000449180603 | Accuracy: tensor([0.9872])\n",
      "Epoch: 1700 | Loss: 0.3525625765323639 | Accuracy: tensor([0.9824])\n",
      "Epoch: 1800 | Loss: 0.34963855147361755 | Accuracy: tensor([0.9880])\n",
      "Epoch: 1900 | Loss: 0.34813374280929565 | Accuracy: tensor([0.9872])\n",
      "Epoch: 2000 | Loss: 0.3465658724308014 | Accuracy: tensor([0.9872])\n",
      "Epoch: 2100 | Loss: 0.34631112217903137 | Accuracy: tensor([0.9840])\n",
      "Epoch: 2200 | Loss: 0.3437805771827698 | Accuracy: tensor([0.9880])\n",
      "Epoch: 2300 | Loss: 0.34220796823501587 | Accuracy: tensor([0.9896])\n",
      "Epoch: 2400 | Loss: 0.34110504388809204 | Accuracy: tensor([0.9896])\n",
      "Epoch: 2500 | Loss: 0.34128808975219727 | Accuracy: tensor([0.9848])\n",
      "Epoch: 2600 | Loss: 0.33940523862838745 | Accuracy: tensor([0.9880])\n",
      "Epoch: 2700 | Loss: 0.33864837884902954 | Accuracy: tensor([0.9880])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfn(y_pred, y_train)\n\u001b[0;32m      5\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m----> 6\u001b[0m \u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\adam.py:445\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    442\u001b[0m     device_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_exp_avgs, device_grads, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m    448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = lossfn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        with th.inference_mode():\n",
    "            y_pred = model(X_test)\n",
    "            \n",
    "        acc = sum(y_test.argmax(dim=1).unsqueeze(dim=1).cpu() == th.argmax(y_pred, dim=1).unsqueeze(dim=1).cpu()) / len(y_pred)\n",
    "        loss = lossfn(y_pred, y_test)\n",
    "        \n",
    "        print(f\"Epoch: {epoch} | Loss: {loss} | Accuracy: {acc}\")\n",
    "        \n",
    "    \n",
    "    optimiser.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
