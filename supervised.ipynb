{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, hidden_size: int = 128, n_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            *[\n",
    "                nn.ReLU() if i % 2 == 0 else nn.Linear(hidden_size, hidden_size) for i in range(n_layers*2)\n",
    "            ],\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_outputs),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        return self.softmax(self.layers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Trading_Price</th>\n",
       "      <th>4H_x1_0</th>\n",
       "      <th>4H_x2_0</th>\n",
       "      <th>4H_x3_0</th>\n",
       "      <th>4H_x4_0</th>\n",
       "      <th>4H_x5_0</th>\n",
       "      <th>4H_x1_1</th>\n",
       "      <th>4H_x2_1</th>\n",
       "      <th>4H_x3_1</th>\n",
       "      <th>...</th>\n",
       "      <th>15m_x2_14</th>\n",
       "      <th>15m_x3_14</th>\n",
       "      <th>15m_x4_14</th>\n",
       "      <th>15m_x5_14</th>\n",
       "      <th>15m_x1_15</th>\n",
       "      <th>15m_x2_15</th>\n",
       "      <th>15m_x3_15</th>\n",
       "      <th>15m_x4_15</th>\n",
       "      <th>15m_x5_15</th>\n",
       "      <th>Lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-20 16:00:00+00:00</td>\n",
       "      <td>1.07259</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-20 16:15:00+00:00</td>\n",
       "      <td>1.07369</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-20 16:30:00+00:00</td>\n",
       "      <td>1.07359</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-20 16:45:00+00:00</td>\n",
       "      <td>1.07353</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-20 17:00:00+00:00</td>\n",
       "      <td>1.07312</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  Trading_Price   4H_x1_0   4H_x2_0   4H_x3_0  \\\n",
       "0  2023-09-20 16:00:00+00:00        1.07259 -0.005189  0.000895 -0.003208   \n",
       "1  2023-09-20 16:15:00+00:00        1.07369 -0.005189  0.000895 -0.003208   \n",
       "2  2023-09-20 16:30:00+00:00        1.07359 -0.005189  0.000895 -0.003208   \n",
       "3  2023-09-20 16:45:00+00:00        1.07353 -0.005189  0.000895 -0.003208   \n",
       "4  2023-09-20 17:00:00+00:00        1.07312 -0.005189  0.000895 -0.003208   \n",
       "\n",
       "    4H_x4_0   4H_x5_0   4H_x1_1   4H_x2_1   4H_x3_1  ...  15m_x2_14  \\\n",
       "0  0.007327  0.000066  0.001187  0.002074  0.000243  ...  -0.000327   \n",
       "1  0.007327  0.000066  0.001187  0.002074  0.000243  ...   0.000131   \n",
       "2  0.007327  0.000066  0.001187  0.002074  0.000243  ...   0.000794   \n",
       "3  0.007327  0.000066  0.001187  0.002074  0.000243  ...  -0.000019   \n",
       "4  0.007327  0.000066  0.001187  0.002074  0.000243  ...   0.000411   \n",
       "\n",
       "   15m_x3_14  15m_x4_14  15m_x5_14  15m_x1_15  15m_x2_15  15m_x3_15  \\\n",
       "0  -0.000411   0.000187   0.000533   0.000196  -0.000009  -0.000047   \n",
       "1   0.000309   0.000318   0.000224  -0.000187  -0.000327  -0.000411   \n",
       "2   0.000150   0.000047   0.001139   0.000000   0.000131   0.000309   \n",
       "3   0.000570   0.000430   0.000168   0.001066   0.000794   0.000150   \n",
       "4   0.000037   0.000327   0.000644  -0.000402  -0.000019   0.000570   \n",
       "\n",
       "   15m_x4_15  15m_x5_15  Lable  \n",
       "0   0.000327   0.000308      1  \n",
       "1   0.000187   0.000533      0  \n",
       "2   0.000318   0.000224      0  \n",
       "3   0.000047   0.001139      0  \n",
       "4   0.000430   0.000168      0  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/training/EURUSD.csv\")\n",
    "df[\"Lable\"] = (df[\"Trading_Price\"].shift(-1)-df[\"Trading_Price\"]).apply(lambda x: 1 if x > 0 else 0)\n",
    "df = df.dropna()\n",
    "features = th.from_numpy(df.drop([\"Date\", \"Trading_Price\", \"Lable\"], axis=1).to_numpy()).to(device=device).type(th.float32)\n",
    "lable = th.from_numpy(df[\"Lable\"].to_numpy()).to(device=device)\n",
    "lable = nn.functional.one_hot(lable, num_classes=2).type(th.float32)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25015, 120]), torch.Size([25015, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, lable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, lable, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Network(\n",
    "    n_inputs=features.shape[1],\n",
    "    n_outputs=2,\n",
    "    hidden_size=256,\n",
    "    n_layers=3\n",
    ").to(device=device)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimiser = th.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=120, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.6931703686714172 | Accuracy: 0.0\n",
      "Epoch: 100 | Loss: 0.6865331530570984 | Accuracy: tensor([0.6667])\n",
      "Epoch: 200 | Loss: 0.6460449695587158 | Accuracy: tensor([0.7320])\n",
      "Epoch: 300 | Loss: 0.6311432719230652 | Accuracy: tensor([0.7615])\n",
      "Epoch: 400 | Loss: 0.6689441204071045 | Accuracy: tensor([0.6564])\n",
      "Epoch: 500 | Loss: 0.6112592816352844 | Accuracy: tensor([0.7882])\n",
      "Epoch: 600 | Loss: 0.7039888501167297 | Accuracy: tensor([0.5896])\n",
      "Epoch: 700 | Loss: 0.6104170680046082 | Accuracy: tensor([0.7575])\n",
      "Epoch: 800 | Loss: 0.5899274945259094 | Accuracy: tensor([0.7892])\n",
      "Epoch: 900 | Loss: 0.583843469619751 | Accuracy: tensor([0.7915])\n",
      "Epoch: 1000 | Loss: 0.6301195621490479 | Accuracy: tensor([0.6944])\n",
      "Epoch: 1100 | Loss: 0.5973305702209473 | Accuracy: tensor([0.7518])\n",
      "Epoch: 1200 | Loss: 0.5814186930656433 | Accuracy: tensor([0.7874])\n",
      "Epoch: 1300 | Loss: 0.6247776746749878 | Accuracy: tensor([0.6984])\n",
      "Epoch: 1400 | Loss: 0.5965507626533508 | Accuracy: tensor([0.7426])\n",
      "Epoch: 1500 | Loss: 0.5960392355918884 | Accuracy: tensor([0.7474])\n",
      "Epoch: 1600 | Loss: 0.5946163535118103 | Accuracy: tensor([0.7512])\n",
      "Epoch: 1700 | Loss: 0.5932998657226562 | Accuracy: tensor([0.7548])\n",
      "Epoch: 1800 | Loss: 0.5924924612045288 | Accuracy: tensor([0.7562])\n",
      "Epoch: 1900 | Loss: 0.5911435484886169 | Accuracy: tensor([0.7592])\n",
      "Epoch: 2000 | Loss: 0.589798092842102 | Accuracy: tensor([0.7619])\n",
      "Epoch: 2100 | Loss: 0.5885145664215088 | Accuracy: tensor([0.7639])\n",
      "Epoch: 2200 | Loss: 0.5873892903327942 | Accuracy: tensor([0.7654])\n",
      "Epoch: 2300 | Loss: 0.5861205458641052 | Accuracy: tensor([0.7683])\n",
      "Epoch: 2400 | Loss: 0.5843833088874817 | Accuracy: tensor([0.7734])\n",
      "Epoch: 2500 | Loss: 0.5829951167106628 | Accuracy: tensor([0.7757])\n",
      "Epoch: 2600 | Loss: 0.5808480978012085 | Accuracy: tensor([0.7792])\n",
      "Epoch: 2700 | Loss: 0.6851078867912292 | Accuracy: tensor([0.6231])\n",
      "Epoch: 2800 | Loss: 0.6600284576416016 | Accuracy: tensor([0.6449])\n",
      "Epoch: 2900 | Loss: 0.6555453538894653 | Accuracy: tensor([0.6542])\n",
      "Epoch: 3000 | Loss: 0.6380863189697266 | Accuracy: tensor([0.6771])\n",
      "Epoch: 3100 | Loss: 0.6426687240600586 | Accuracy: tensor([0.6786])\n",
      "Epoch: 3200 | Loss: 0.6246591210365295 | Accuracy: tensor([0.7111])\n",
      "Epoch: 3300 | Loss: 0.6345816850662231 | Accuracy: tensor([0.6957])\n",
      "Epoch: 3400 | Loss: 0.6241794228553772 | Accuracy: tensor([0.7110])\n",
      "Epoch: 3500 | Loss: 0.6289263367652893 | Accuracy: tensor([0.7015])\n",
      "Epoch: 3600 | Loss: 0.6172397136688232 | Accuracy: tensor([0.7224])\n",
      "Epoch: 3700 | Loss: 0.6231013536453247 | Accuracy: tensor([0.7126])\n",
      "Epoch: 3800 | Loss: 0.616274356842041 | Accuracy: tensor([0.7239])\n",
      "Epoch: 3900 | Loss: 0.617586076259613 | Accuracy: tensor([0.7198])\n",
      "Epoch: 4000 | Loss: 0.6156017184257507 | Accuracy: tensor([0.7218])\n",
      "Epoch: 4100 | Loss: 0.6105934381484985 | Accuracy: tensor([0.7320])\n",
      "Epoch: 4200 | Loss: 0.6168133020401001 | Accuracy: tensor([0.7184])\n",
      "Epoch: 4300 | Loss: 0.6044143438339233 | Accuracy: tensor([0.7392])\n",
      "Epoch: 4400 | Loss: 0.6191003918647766 | Accuracy: tensor([0.7133])\n",
      "Epoch: 4500 | Loss: 0.5983164310455322 | Accuracy: tensor([0.7484])\n",
      "Epoch: 4600 | Loss: 0.6250390410423279 | Accuracy: tensor([0.6979])\n",
      "Epoch: 4700 | Loss: 0.5893707871437073 | Accuracy: tensor([0.7540])\n",
      "Epoch: 4800 | Loss: 0.5872047543525696 | Accuracy: tensor([0.7545])\n",
      "Epoch: 4900 | Loss: 0.5891720652580261 | Accuracy: tensor([0.7543])\n",
      "Epoch: 5000 | Loss: 0.5883539319038391 | Accuracy: tensor([0.7628])\n",
      "Epoch: 5100 | Loss: 0.587243378162384 | Accuracy: tensor([0.7575])\n",
      "Epoch: 5200 | Loss: 0.592158854007721 | Accuracy: tensor([0.7466])\n",
      "Epoch: 5300 | Loss: 0.5867867469787598 | Accuracy: tensor([0.7576])\n",
      "Epoch: 5400 | Loss: 0.5860399007797241 | Accuracy: tensor([0.7557])\n",
      "Epoch: 5500 | Loss: 0.5873091220855713 | Accuracy: tensor([0.7561])\n",
      "Epoch: 5600 | Loss: 0.5913593173027039 | Accuracy: tensor([0.7540])\n",
      "Epoch: 5700 | Loss: 0.5811296701431274 | Accuracy: tensor([0.7648])\n",
      "Epoch: 5800 | Loss: 0.5818741917610168 | Accuracy: tensor([0.7653])\n",
      "Epoch: 5900 | Loss: 0.5987093448638916 | Accuracy: tensor([0.7383])\n",
      "Epoch: 6000 | Loss: 0.5793488025665283 | Accuracy: tensor([0.7682])\n",
      "Epoch: 6100 | Loss: 0.5990416407585144 | Accuracy: tensor([0.7245])\n",
      "Epoch: 6200 | Loss: 0.6261884570121765 | Accuracy: tensor([0.6894])\n",
      "Epoch: 6300 | Loss: 0.6367404460906982 | Accuracy: tensor([0.6791])\n",
      "Epoch: 6400 | Loss: 0.6300249099731445 | Accuracy: tensor([0.6912])\n",
      "Epoch: 6500 | Loss: 0.6237137913703918 | Accuracy: tensor([0.7026])\n",
      "Epoch: 6600 | Loss: 0.6193105578422546 | Accuracy: tensor([0.7107])\n",
      "Epoch: 6700 | Loss: 0.6163179278373718 | Accuracy: tensor([0.7188])\n",
      "Epoch: 6800 | Loss: 0.6145485043525696 | Accuracy: tensor([0.7221])\n",
      "Epoch: 6900 | Loss: 0.6127904653549194 | Accuracy: tensor([0.7257])\n",
      "Epoch: 7000 | Loss: 0.6115847229957581 | Accuracy: tensor([0.7286])\n",
      "Epoch: 7100 | Loss: 0.6105456948280334 | Accuracy: tensor([0.7292])\n",
      "Epoch: 7200 | Loss: 0.6099672317504883 | Accuracy: tensor([0.7295])\n",
      "Epoch: 7300 | Loss: 0.608910858631134 | Accuracy: tensor([0.7311])\n",
      "Epoch: 7400 | Loss: 0.6081918478012085 | Accuracy: tensor([0.7326])\n",
      "Epoch: 7500 | Loss: 0.608205258846283 | Accuracy: tensor([0.7339])\n",
      "Epoch: 7600 | Loss: 0.6075817346572876 | Accuracy: tensor([0.7355])\n",
      "Epoch: 7700 | Loss: 0.6063510179519653 | Accuracy: tensor([0.7358])\n",
      "Epoch: 7800 | Loss: 0.6051608920097351 | Accuracy: tensor([0.7348])\n",
      "Epoch: 7900 | Loss: 0.6040236949920654 | Accuracy: tensor([0.7354])\n",
      "Epoch: 8000 | Loss: 0.6035453081130981 | Accuracy: tensor([0.7367])\n",
      "Epoch: 8100 | Loss: 0.6036244630813599 | Accuracy: tensor([0.7375])\n",
      "Epoch: 8200 | Loss: 0.6035330891609192 | Accuracy: tensor([0.7374])\n",
      "Epoch: 8300 | Loss: 0.6032347083091736 | Accuracy: tensor([0.7379])\n",
      "Epoch: 8400 | Loss: 0.6033923625946045 | Accuracy: tensor([0.7377])\n",
      "Epoch: 8500 | Loss: 0.603485107421875 | Accuracy: tensor([0.7371])\n",
      "Epoch: 8600 | Loss: 0.6030178070068359 | Accuracy: tensor([0.7380])\n",
      "Epoch: 8700 | Loss: 0.6029448509216309 | Accuracy: tensor([0.7372])\n",
      "Epoch: 8800 | Loss: 0.602874755859375 | Accuracy: tensor([0.7376])\n",
      "Epoch: 8900 | Loss: 0.6027418971061707 | Accuracy: tensor([0.7366])\n",
      "Epoch: 9000 | Loss: 0.6028013229370117 | Accuracy: tensor([0.7353])\n",
      "Epoch: 9100 | Loss: 0.6028232574462891 | Accuracy: tensor([0.7339])\n",
      "Epoch: 9200 | Loss: 0.6031085252761841 | Accuracy: tensor([0.7326])\n",
      "Epoch: 9300 | Loss: 0.6027224659919739 | Accuracy: tensor([0.7343])\n",
      "Epoch: 9400 | Loss: 0.6028383374214172 | Accuracy: tensor([0.7341])\n",
      "Epoch: 9500 | Loss: 0.6026082634925842 | Accuracy: tensor([0.7337])\n",
      "Epoch: 9600 | Loss: 0.6025145649909973 | Accuracy: tensor([0.7344])\n",
      "Epoch: 9700 | Loss: 0.6028037667274475 | Accuracy: tensor([0.7345])\n",
      "Epoch: 9800 | Loss: 0.6033192276954651 | Accuracy: tensor([0.7347])\n",
      "Epoch: 9900 | Loss: 0.6033975481987 | Accuracy: tensor([0.7339])\n",
      "Epoch: 10000 | Loss: 0.6032647490501404 | Accuracy: tensor([0.7340])\n",
      "Epoch: 10100 | Loss: 0.6035429239273071 | Accuracy: tensor([0.7331])\n",
      "Epoch: 10200 | Loss: 0.6035730838775635 | Accuracy: tensor([0.7327])\n",
      "Epoch: 10300 | Loss: 0.6035032868385315 | Accuracy: tensor([0.7330])\n",
      "Epoch: 10400 | Loss: 0.6031792759895325 | Accuracy: tensor([0.7338])\n",
      "Epoch: 10500 | Loss: 0.6035035848617554 | Accuracy: tensor([0.7326])\n",
      "Epoch: 10600 | Loss: 0.6033983826637268 | Accuracy: tensor([0.7330])\n",
      "Epoch: 10700 | Loss: 0.6034563779830933 | Accuracy: tensor([0.7323])\n",
      "Epoch: 10800 | Loss: 0.6034153699874878 | Accuracy: tensor([0.7325])\n",
      "Epoch: 10900 | Loss: 0.6035868525505066 | Accuracy: tensor([0.7318])\n",
      "Epoch: 11000 | Loss: 0.6037024259567261 | Accuracy: tensor([0.7323])\n",
      "Epoch: 11100 | Loss: 0.6039810180664062 | Accuracy: tensor([0.7314])\n",
      "Epoch: 11200 | Loss: 0.603922426700592 | Accuracy: tensor([0.7308])\n",
      "Epoch: 11300 | Loss: 0.6043365001678467 | Accuracy: tensor([0.7306])\n",
      "Epoch: 11400 | Loss: 0.6048992872238159 | Accuracy: tensor([0.7290])\n",
      "Epoch: 11500 | Loss: 0.6036131381988525 | Accuracy: tensor([0.7303])\n",
      "Epoch: 11600 | Loss: 0.6047905683517456 | Accuracy: tensor([0.7278])\n",
      "Epoch: 11700 | Loss: 0.6046797037124634 | Accuracy: tensor([0.7267])\n",
      "Epoch: 11800 | Loss: 0.6036022305488586 | Accuracy: tensor([0.7296])\n",
      "Epoch: 11900 | Loss: 0.6038610935211182 | Accuracy: tensor([0.7278])\n",
      "Epoch: 12000 | Loss: 0.604153037071228 | Accuracy: tensor([0.7269])\n",
      "Epoch: 12100 | Loss: 0.6035997271537781 | Accuracy: tensor([0.7277])\n",
      "Epoch: 12200 | Loss: 0.6043040156364441 | Accuracy: tensor([0.7263])\n",
      "Epoch: 12300 | Loss: 0.6060277223587036 | Accuracy: tensor([0.7228])\n",
      "Epoch: 12400 | Loss: 0.6059916615486145 | Accuracy: tensor([0.7227])\n",
      "Epoch: 12500 | Loss: 0.6073392629623413 | Accuracy: tensor([0.7199])\n",
      "Epoch: 12600 | Loss: 0.6045331954956055 | Accuracy: tensor([0.7240])\n",
      "Epoch: 12700 | Loss: 0.6039774417877197 | Accuracy: tensor([0.7262])\n",
      "Epoch: 12800 | Loss: 0.6064491868019104 | Accuracy: tensor([0.7206])\n",
      "Epoch: 12900 | Loss: 0.6062495708465576 | Accuracy: tensor([0.7208])\n",
      "Epoch: 13000 | Loss: 0.6052562594413757 | Accuracy: tensor([0.7250])\n",
      "Epoch: 13100 | Loss: 0.6065574288368225 | Accuracy: tensor([0.7194])\n",
      "Epoch: 13200 | Loss: 0.6049075722694397 | Accuracy: tensor([0.7241])\n",
      "Epoch: 13300 | Loss: 0.6060656309127808 | Accuracy: tensor([0.7222])\n",
      "Epoch: 13400 | Loss: 0.6059486269950867 | Accuracy: tensor([0.7211])\n",
      "Epoch: 13500 | Loss: 0.607741117477417 | Accuracy: tensor([0.7180])\n",
      "Epoch: 13600 | Loss: 0.6067139506340027 | Accuracy: tensor([0.7225])\n",
      "Epoch: 13700 | Loss: 0.6095030903816223 | Accuracy: tensor([0.7145])\n",
      "Epoch: 13800 | Loss: 0.6049622893333435 | Accuracy: tensor([0.7251])\n",
      "Epoch: 13900 | Loss: 0.6077118515968323 | Accuracy: tensor([0.7173])\n",
      "Epoch: 14000 | Loss: 0.6059539318084717 | Accuracy: tensor([0.7203])\n",
      "Epoch: 14100 | Loss: 0.607655942440033 | Accuracy: tensor([0.7184])\n",
      "Epoch: 14200 | Loss: 0.5969589948654175 | Accuracy: tensor([0.7313])\n",
      "Epoch: 14300 | Loss: 0.5995117425918579 | Accuracy: tensor([0.7256])\n",
      "Epoch: 14400 | Loss: 0.6053770184516907 | Accuracy: tensor([0.7158])\n",
      "Epoch: 14500 | Loss: 0.6003432869911194 | Accuracy: tensor([0.7210])\n",
      "Epoch: 14600 | Loss: 0.6037836074829102 | Accuracy: tensor([0.7168])\n",
      "Epoch: 14700 | Loss: 0.6055749654769897 | Accuracy: tensor([0.7155])\n",
      "Epoch: 14800 | Loss: 0.5999928116798401 | Accuracy: tensor([0.7208])\n",
      "Epoch: 14900 | Loss: 0.6058077812194824 | Accuracy: tensor([0.7141])\n",
      "Epoch: 15000 | Loss: 0.6080939173698425 | Accuracy: tensor([0.7112])\n",
      "Epoch: 15100 | Loss: 0.6016630530357361 | Accuracy: tensor([0.7172])\n",
      "Epoch: 15200 | Loss: 0.608087956905365 | Accuracy: tensor([0.7102])\n",
      "Epoch: 15300 | Loss: 0.609292209148407 | Accuracy: tensor([0.7088])\n",
      "Epoch: 15400 | Loss: 0.6021872758865356 | Accuracy: tensor([0.7172])\n",
      "Epoch: 15500 | Loss: 0.6090653538703918 | Accuracy: tensor([0.7098])\n",
      "Epoch: 15600 | Loss: 0.6108322143554688 | Accuracy: tensor([0.7075])\n",
      "Epoch: 15700 | Loss: 0.6028441786766052 | Accuracy: tensor([0.7151])\n",
      "Epoch: 15800 | Loss: 0.6075764894485474 | Accuracy: tensor([0.7119])\n",
      "Epoch: 15900 | Loss: 0.6103029847145081 | Accuracy: tensor([0.7086])\n",
      "Epoch: 16000 | Loss: 0.6057313680648804 | Accuracy: tensor([0.7102])\n",
      "Epoch: 16100 | Loss: 0.6102745532989502 | Accuracy: tensor([0.7071])\n",
      "Epoch: 16200 | Loss: 0.6114432215690613 | Accuracy: tensor([0.7062])\n",
      "Epoch: 16300 | Loss: 0.6047111749649048 | Accuracy: tensor([0.7143])\n",
      "Epoch: 16400 | Loss: 0.6060640811920166 | Accuracy: tensor([0.7118])\n",
      "Epoch: 16500 | Loss: 0.6068095564842224 | Accuracy: tensor([0.7114])\n",
      "Epoch: 16600 | Loss: 0.6077990531921387 | Accuracy: tensor([0.7093])\n",
      "Epoch: 16700 | Loss: 0.6087824702262878 | Accuracy: tensor([0.7083])\n",
      "Epoch: 16800 | Loss: 0.6098793148994446 | Accuracy: tensor([0.7075])\n",
      "Epoch: 16900 | Loss: 0.6104254722595215 | Accuracy: tensor([0.7056])\n",
      "Epoch: 17000 | Loss: 0.6108265519142151 | Accuracy: tensor([0.7046])\n",
      "Epoch: 17100 | Loss: 0.6118462681770325 | Accuracy: tensor([0.7047])\n",
      "Epoch: 17200 | Loss: 0.6211921572685242 | Accuracy: tensor([0.6938])\n",
      "Epoch: 17300 | Loss: 0.6162697672843933 | Accuracy: tensor([0.6987])\n",
      "Epoch: 17400 | Loss: 0.6196061372756958 | Accuracy: tensor([0.6953])\n",
      "Epoch: 17500 | Loss: 0.6140424013137817 | Accuracy: tensor([0.7006])\n",
      "Epoch: 17600 | Loss: 0.6186140775680542 | Accuracy: tensor([0.6959])\n",
      "Epoch: 17700 | Loss: 0.6197517514228821 | Accuracy: tensor([0.6948])\n",
      "Epoch: 17800 | Loss: 0.6150656342506409 | Accuracy: tensor([0.6989])\n",
      "Epoch: 17900 | Loss: 0.6157652139663696 | Accuracy: tensor([0.6994])\n",
      "Epoch: 18000 | Loss: 0.6208994388580322 | Accuracy: tensor([0.6931])\n",
      "Epoch: 18100 | Loss: 0.6211865544319153 | Accuracy: tensor([0.6931])\n",
      "Epoch: 18200 | Loss: 0.6198325157165527 | Accuracy: tensor([0.6953])\n",
      "Epoch: 18300 | Loss: 0.6161227822303772 | Accuracy: tensor([0.6984])\n",
      "Epoch: 18400 | Loss: 0.6191993951797485 | Accuracy: tensor([0.6948])\n",
      "Epoch: 18500 | Loss: 0.6169618368148804 | Accuracy: tensor([0.6980])\n",
      "Epoch: 18600 | Loss: 0.6201586723327637 | Accuracy: tensor([0.6951])\n",
      "Epoch: 18700 | Loss: 0.618640124797821 | Accuracy: tensor([0.6954])\n",
      "Epoch: 18800 | Loss: 0.6186140179634094 | Accuracy: tensor([0.6967])\n",
      "Epoch: 18900 | Loss: 0.6252803802490234 | Accuracy: tensor([0.6872])\n",
      "Epoch: 19000 | Loss: 0.6256717443466187 | Accuracy: tensor([0.6873])\n",
      "Epoch: 19100 | Loss: 0.6202548146247864 | Accuracy: tensor([0.6934])\n",
      "Epoch: 19200 | Loss: 0.6212868690490723 | Accuracy: tensor([0.6924])\n",
      "Epoch: 19300 | Loss: 0.6205245852470398 | Accuracy: tensor([0.6925])\n",
      "Epoch: 19400 | Loss: 0.6201724410057068 | Accuracy: tensor([0.6940])\n",
      "Epoch: 19500 | Loss: 0.6221339702606201 | Accuracy: tensor([0.6922])\n",
      "Epoch: 19600 | Loss: 0.6226651072502136 | Accuracy: tensor([0.6905])\n",
      "Epoch: 19700 | Loss: 0.6182178258895874 | Accuracy: tensor([0.6957])\n",
      "Epoch: 19800 | Loss: 0.6232163310050964 | Accuracy: tensor([0.6901])\n",
      "Epoch: 19900 | Loss: 0.6217341423034668 | Accuracy: tensor([0.6928])\n",
      "Epoch: 20000 | Loss: 0.6231679916381836 | Accuracy: tensor([0.6903])\n",
      "Epoch: 20100 | Loss: 0.6228935718536377 | Accuracy: tensor([0.6906])\n",
      "Epoch: 20200 | Loss: 0.6208001375198364 | Accuracy: tensor([0.6925])\n",
      "Epoch: 20300 | Loss: 0.6202720999717712 | Accuracy: tensor([0.6941])\n",
      "Epoch: 20400 | Loss: 0.623546302318573 | Accuracy: tensor([0.6895])\n",
      "Epoch: 20500 | Loss: 0.6221485137939453 | Accuracy: tensor([0.6909])\n",
      "Epoch: 20600 | Loss: 0.6239138841629028 | Accuracy: tensor([0.6893])\n",
      "Epoch: 20700 | Loss: 0.624651312828064 | Accuracy: tensor([0.6899])\n",
      "Epoch: 20800 | Loss: 0.6234402060508728 | Accuracy: tensor([0.6898])\n",
      "Epoch: 20900 | Loss: 0.6223644018173218 | Accuracy: tensor([0.6913])\n",
      "Epoch: 21000 | Loss: 0.6234381198883057 | Accuracy: tensor([0.6905])\n",
      "Epoch: 21100 | Loss: 0.6226568222045898 | Accuracy: tensor([0.6900])\n",
      "Epoch: 21200 | Loss: 0.6264541745185852 | Accuracy: tensor([0.6863])\n",
      "Epoch: 21300 | Loss: 0.6230329275131226 | Accuracy: tensor([0.6912])\n",
      "Epoch: 21400 | Loss: 0.6223040819168091 | Accuracy: tensor([0.6916])\n",
      "Epoch: 21500 | Loss: 0.6369084119796753 | Accuracy: tensor([0.6765])\n",
      "Epoch: 21600 | Loss: 0.6223403811454773 | Accuracy: tensor([0.6917])\n",
      "Epoch: 21700 | Loss: 0.6215705275535583 | Accuracy: tensor([0.6923])\n",
      "Epoch: 21800 | Loss: 0.6266462206840515 | Accuracy: tensor([0.6869])\n",
      "Epoch: 21900 | Loss: 0.6239990592002869 | Accuracy: tensor([0.6894])\n",
      "Epoch: 22000 | Loss: 0.6262977719306946 | Accuracy: tensor([0.6874])\n",
      "Epoch: 22100 | Loss: 0.6276969909667969 | Accuracy: tensor([0.6859])\n",
      "Epoch: 22200 | Loss: 0.6225748062133789 | Accuracy: tensor([0.6906])\n",
      "Epoch: 22300 | Loss: 0.6294505596160889 | Accuracy: tensor([0.6838])\n",
      "Epoch: 22400 | Loss: 0.6260652542114258 | Accuracy: tensor([0.6874])\n",
      "Epoch: 22500 | Loss: 0.6240247488021851 | Accuracy: tensor([0.6899])\n",
      "Epoch: 22600 | Loss: 0.6227038502693176 | Accuracy: tensor([0.6903])\n",
      "Epoch: 22700 | Loss: 0.6234628558158875 | Accuracy: tensor([0.6898])\n",
      "Epoch: 22800 | Loss: 0.6286049485206604 | Accuracy: tensor([0.6850])\n",
      "Epoch: 22900 | Loss: 0.6272568702697754 | Accuracy: tensor([0.6858])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X_test)\n\u001b[0;32m     15\u001b[0m confident_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_pred\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m confidence_threshold)[\u001b[38;5;241m0\u001b[39m]   \n\u001b[1;32m---> 16\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(y_test\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()[confident_indices] \u001b[38;5;241m==\u001b[39m th\u001b[38;5;241m.\u001b[39margmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()[confident_indices]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred[confident_indices]), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfn(y_pred, y_test)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\_tensor.py:940\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m    932\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    939\u001b[0m     )\n\u001b[1;32m--> 940\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X_train)\n",
    "    optimiser.zero_grad()\n",
    "    loss = lossfn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    th.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimiser.step()\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        with th.inference_mode():\n",
    "            y_pred = model(X_test)\n",
    "        confident_indices = np.where(y_pred.cpu() >= confidence_threshold)[0]   \n",
    "        acc = sum(y_test.argmax(dim=1).unsqueeze(dim=1).cpu()[confident_indices] == th.argmax(y_pred, dim=1).unsqueeze(dim=1).cpu()[confident_indices]) / max(len(y_pred[confident_indices]), 1)\n",
    "        loss = lossfn(y_pred, y_test)\n",
    "        \n",
    "        print(f\"Epoch: {epoch} | Loss: {loss} | Accuracy: {acc}\")\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.drop([\"Date\", \"Trading_Price\", \"Lable\"], axis=1).columns.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 120])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15m_x1_2'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(model, \"models/supervised_01.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
