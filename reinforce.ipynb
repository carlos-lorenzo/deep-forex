{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# gym\n",
    "import gymnasium as gym\n",
    "\n",
    "from forexgym.envs import DiscreteActionEnvironment\n",
    "from forexgym.utils import Query, CurrencyPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, hidden_size: int = 128, continuous_actions: bool = False):\n",
    "        super(Policy, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_outputs)\n",
    "        )\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_pred = self.layers(x)\n",
    "        return F.softmax(y_pred, dim=1)\n",
    "    \n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        m = Categorical(self.forward(state))\n",
    "        action = m.sample()\n",
    "        self.log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def act_secure(self, state: np.ndarray) -> int:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        action = torch.argmax(probs).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_processor(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n",
    "    df[\"x1\"] = ((df[\"Close\"].shift(-1) - df[\"Close\"]) / df[\"Close\"]).shift(1) \n",
    "    df[\"x2\"] = ((df[\"High\"].shift(-1) - df[\"High\"]) / df[\"High\"]).shift(1) \n",
    "    df[\"x3\"] = ((df[\"Low\"].shift(-1) - df[\"Low\"]) / df[\"Low\"]).shift(1) \n",
    "    df[\"x4\"] = (df[\"High\"] - df[\"Close\"]) / df[\"Close\"] \n",
    "    df[\"x5\"] = (df[\"Close\"] - df[\"Low\"]) / df[\"Close\"] \n",
    "    \n",
    "    return df.drop([\"Date\", \"Open\", \"High\", \"Low\", \"Close\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genrating EURUSD dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated EURUSD dataset.\n"
     ]
    }
   ],
   "source": [
    "ticker = \"EURUSD\"\n",
    "#timeframes = [\"1m\", \"5m\", \"15m\", \"30m\", \"1H\", \"4H\", \"1D\"]\n",
    "timeframes = [\"4H\", \"1H\", \"15m\"]\n",
    "\n",
    "query = Query(episode_length=256, trading_timeframe=\"1H\", trading_column=\"Close\")\n",
    "query.add_query(\n",
    "    timeframe=\"1H\",\n",
    "    window_size=16,\n",
    "    data_processor=article_processor\n",
    ")\n",
    "# dataset = pair.generate_dataset(query)\n",
    "\n",
    "env = DiscreteActionEnvironment(\n",
    "    currency_tickers={\"EURUSD\": timeframes},\n",
    "    query=query,\n",
    "    reward_type=\"continuous\",\n",
    "    reward_multiplier=1e3,\n",
    "    episode_length=256,\n",
    "    allow_holding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\dev\\Repos\\forex-gym\\forexgym\\envs\\base_environment.py:55\u001b[0m, in \u001b[0;36mBaseEnvironment.reset\u001b[1;34m(self, seed, options, *args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_episode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m() \u001b[38;5;66;03m# Episode initialization handled by Episode class\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[0;32m     58\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_info()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m ENV \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#env = gym.make(ENV)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m state, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m policy \u001b[38;5;241m=\u001b[39m Policy(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\dev\\Repos\\forex-gym\\forexgym\\envs\\base_environment.py:55\u001b[0m, in \u001b[0;36mBaseEnvironment.reset\u001b[1;34m(self, seed, options, *args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_episode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m() \u001b[38;5;66;03m# Episode initialization handled by Episode class\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[0;32m     58\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_info()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close() # type: ignore  # noqa: F821\n",
    "except: # type: ignore  # noqa: E722, F821\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "ENV = \"LunarLander-v2\"\n",
    "#env = gym.make(ENV)\n",
    "state, info = env.reset()\n",
    "\n",
    "policy = Policy(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "optimiser = optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "#MAX_TIME_STEPS = env.spec.max_episode_steps\n",
    "#REWARD_THRESHOLD = env.spec.reward_threshold\n",
    "N_EPISODES = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode() -> None:\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(MAX_TIME_STEPS):\n",
    "        action = policy.act(state)\n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + DISCOUNT_FACTOR * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimiser.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum().to(device)\n",
    "    policy_loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    \n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    n_completions = 0\n",
    "    \n",
    "    for ep in range(N_EPISODES):\n",
    "        \n",
    "        del policy.rewards[:]\n",
    "        del policy.log_probs[:]\n",
    "        \n",
    "        sample_episode()\n",
    "        finish_episode()\n",
    "        \n",
    "        if ep % 25 == 0:\n",
    "            total_rewards = sum(policy.rewards)\n",
    "            \n",
    "            n_completions += total_rewards > REWARD_THRESHOLD\n",
    "                \n",
    "            \n",
    "            print(f\"Episode: {ep} | Reward: {sum(policy.rewards)}\")\n",
    "            \n",
    "            if n_completions >= 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Reward: 26.95846946250927\n",
      "Episode: 25 | Reward: -54.45134080111597\n",
      "Episode: 50 | Reward: 231.7256943072305\n",
      "Episode: 75 | Reward: 141.33193941359775\n",
      "Episode: 100 | Reward: 229.32840093026783\n",
      "Episode: 125 | Reward: 146.9576331301402\n",
      "Episode: 150 | Reward: 23.92448867696136\n",
      "Episode: 175 | Reward: 111.80948949583285\n",
      "Episode: 200 | Reward: 108.67486720736593\n",
      "Episode: 225 | Reward: 52.47914284634612\n",
      "Episode: 250 | Reward: 173.72561280570642\n",
      "Episode: 275 | Reward: 23.948937827469678\n",
      "Episode: 300 | Reward: 258.90483039639196\n",
      "Episode: 325 | Reward: -119.35525067411466\n",
      "Episode: 350 | Reward: -27.96291973655545\n",
      "Episode: 375 | Reward: -79.49269247361633\n",
      "Episode: 400 | Reward: -31.523365333090737\n",
      "Episode: 425 | Reward: -26.757083332711943\n",
      "Episode: 450 | Reward: 182.23914224650062\n",
      "Episode: 475 | Reward: -51.33235046745686\n",
      "Episode: 500 | Reward: -86.02395466355523\n",
      "Episode: 525 | Reward: -253.55422953620823\n",
      "Episode: 550 | Reward: -48.22400618094291\n",
      "Episode: 575 | Reward: -75.77218731170261\n",
      "Episode: 600 | Reward: 68.93461916909685\n",
      "Episode: 625 | Reward: 101.96368228881275\n",
      "Episode: 650 | Reward: 23.652841518329467\n",
      "Episode: 675 | Reward: 250.39244871578708\n",
      "Episode: 700 | Reward: 50.662202768013884\n",
      "Episode: 725 | Reward: 41.472657147499234\n",
      "Episode: 750 | Reward: 228.69501601591804\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close() # type: ignore  # noqa: F821\n",
    "except: # type: ignore  # noqa: E722, F821\n",
    "    pass\n",
    "\n",
    "env = gym.make(ENV, render_mode=\"human\")\n",
    "\n",
    "\n",
    "def benchmark():\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(MAX_TIME_STEPS):\n",
    "        action = policy.act_secure(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[640], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[637], line 10\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark\u001b[39m():\n\u001b[1;32m---> 10\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_TIME_STEPS):\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:59\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:461\u001b[0m, in \u001b[0;36mLunarLander.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrawlist \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegs\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m], {}\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\anaconda3\\envs\\ml\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:785\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 785\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy, \"lunar_lander_reinforce.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
